{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# display settings & code formatting\n",
    "pd.options.display.max_columns = 999\n",
    "%matplotlib inline\n",
    "%load_ext nb_black\n",
    "\n",
    "# project paths\n",
    "project_root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "\n",
    "data_path = os.path.join(project_root_dir, \"data\")\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "image_path = os.path.join(project_root_dir, \"images\")\n",
    "os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "# function for loading data\n",
    "def load_data(filename, data_path=data_path):\n",
    "    csv_path = os.path.join(data_path, filename)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# function for saving data as csv file\n",
    "def save_dataframe(df, filename, file_path=data_path):\n",
    "    path = os.path.join(file_path, filename)\n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data(\"train.csv\")\n",
    "test = load_data(\"test.csv\")\n",
    "stores = load_data(\"stores.csv\")\n",
    "features = load_data(\"features.csv\")\n",
    "sample_submission = load_data(\"sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data shape: {}\".format(train.shape))\n",
    "print(\"Test Data shape: {}\".format(test.shape))\n",
    "print(\"Stores Data shape: {}\".format(stores.shape))\n",
    "print(\"Features Data shape: {}\".format(features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-husband",
   "metadata": {},
   "source": [
    "## Distribution of weekly Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=train[\"Weekly_Sales\"]))\n",
    "fig.update_layout(\n",
    "    title=\"Weekly Sales at Walmart\", xaxis_title=\"Weekly Sales\", yaxis_title=\"Count\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-immunology",
   "metadata": {},
   "source": [
    "We can see that the distribution is heavily right skewed which means there are many stores with low or medium weekly sales and few stores with very large sales. It might be due to the fact that, some stores are small in size and some are bigger. The locality of the store also matter. In smaller town we expect the sales to be lower compared to main places in urban areas. The density of the population at that area can also be a factor. In fact, the stores csv files contains information about the size of each stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-cutting",
   "metadata": {},
   "source": [
    "Let's also suppliment the histogram with the numerical summary to get a better understanding of this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Weekly_Sales\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-dispute",
   "metadata": {},
   "source": [
    "The mean weekly sales is around $\\$16000$ and have an standard deviation of $\\$22700$ which is a lot. which again indicate that there is a lot of variabilty in weekly sales. The median sales also higlight this fact as it is around $\\$7500$, which is even less than half of the mean value. The maximum sales is $\\$693000$. And the minimum value is -$\\$4988$, which might be due to more items/dollar amount were returned than sold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-pressing",
   "metadata": {},
   "source": [
    "One better way to visualize this distribution is by taking the log of the weekly sales. Taking the log will make the distribution looks more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=np.log10(train[\"Weekly_Sales\"])))\n",
    "fig.update_layout(\n",
    "    title=\"Log of Weekly Sales at Walmart\",\n",
    "    xaxis_title=\"Log base 10 of Weekly Sales\",\n",
    "    yaxis_title=\"Count\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(train[\"Weekly_Sales\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-writing",
   "metadata": {},
   "source": [
    "You can see that we are geeting some \"division by zero error\" and also the mean is -inf and some nan value. It's because we can not take the log of 0 and negative numbers. In python and other statistical libraries like R, the log of 0 is -inf and negative numbers is nan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-status",
   "metadata": {},
   "source": [
    "So, let's fix this error by only taking the log of values that are greater or equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=np.log10(train[train[\"Weekly_Sales\"] >= 1][\"Weekly_Sales\"]))\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Log of Weekly Sales at Walmart\",\n",
    "    xaxis_title=\"Log base 10 of Weekly Sales\",\n",
    "    yaxis_title=\"Count\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(train[train[\"Weekly_Sales\"] >= 1][\"Weekly_Sales\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-alloy",
   "metadata": {},
   "source": [
    "We can see that after taking the log, the distribution mostly looks normal but also with a bit of negativly skewed. And  the mean and median is somewhere around 4. We can take the anti log of these values to get the values back in the original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of weekly sales:\", np.round(10 ** 3.7, 0))\n",
    "print(\"Median of weekly sales:\", np.round(10 ** 3.9, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-shanghai",
   "metadata": {},
   "source": [
    "The median is still around same as before but the mean has reduced a lot from $\\$16000$ to $\\$5000$. Taking the log reduced the effects of extreme values. And the median is more than the mean is due to that negativly skewed distribution. So a better measure of central tendency for this weekly sales data is median as it is not affected by extreme values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-premiere",
   "metadata": {},
   "source": [
    "## Merge the stores data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-calibration",
   "metadata": {},
   "source": [
    "Let's meger the stores data with the training and test set and look at the weekly sales data by the store size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, stores, how=\"left\", on=\"Store\")\n",
    "test = pd.merge(test, stores, how=\"left\", on=\"Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-forum",
   "metadata": {},
   "source": [
    "## Distribution of weekly sales by store type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Type\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-shell",
   "metadata": {},
   "source": [
    "51% of the data is from type A store, 39% from type B and 10% from type c. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"Type\")[\"Size\"].mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-bridal",
   "metadata": {},
   "source": [
    "We can see that the size of the store A is bigger than B and store B is bigger than C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_sales_type = (\n",
    "    train.groupby(\"Type\")[\"Weekly_Sales\"].median().reset_index().round(0)\n",
    ")\n",
    "median_sales_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=median_sales_type[\"Type\"], y=median_sales_type[\"Weekly_Sales\"]))\n",
    "fig.update_layout(\n",
    "    title=\"Median Weekly Sales BY Store Type\",\n",
    "    xaxis_title=\"Store Type\",\n",
    "    yaxis_title=\"Median Weekly Sales\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-effect",
   "metadata": {},
   "source": [
    "As we expect the bigger store has more sales than smaller stores. Now, let's also make a side by side boxplot which is best when we want to find a relationship between a categorical and a numerical feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(train, x=\"Type\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-accident",
   "metadata": {},
   "source": [
    "We can see that there are lots of outliers in the data. So, lets first create a new column which is the log base 10 of weekly sales as we might need it again later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take log10 of weekly sales where sale >= 1 otherwise 0\n",
    "train[\"log10_Weekly_Sales\"] = np.where(\n",
    "    train[\"Weekly_Sales\"] >= 1, np.log10(train[\"Weekly_Sales\"]), 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(train, x=\"Type\", y=\"log10_Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-trace",
   "metadata": {},
   "source": [
    "It clear from the above figure that there are more outliers in A and B type stores than the c type, which means that there are few A and B type stores which is peformaing way more poorly than rest of the stores in their group and the variability in weekly sales is much higher in C type stores than the type A and B stores. Let's isolate the outlier stores in A and B type stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type A And B Stores with very low weekly sales\n",
    "train[(train[\"Type\"].isin([\"A\", \"B\"])) & (train[\"log10_Weekly_Sales\"] <= 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-standing",
   "metadata": {},
   "source": [
    "We can dive more deeper into these data to understand what are the reason for this by combining these data with some other data or talk with someone who manages these stores to get some context behind the failure of these stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-promotion",
   "metadata": {},
   "source": [
    "Let's also make a scatter plot to see if we can find some important trend in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(train, x=\"Size\", y=\"log10_Weekly_Sales\", color=\"Type\", trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(train, x=\"Size\", y=\"Weekly_Sales\", color=\"Type\", trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"Weekly_Sales\", \"Size\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-structure",
   "metadata": {},
   "source": [
    "Based on the plot and the correlation matrix, we can see that there is a very weak correlation between the size of the stores and the weekly sales they generate. We can't say that if the size of the stores increase there sales also increase or decreases. Both are not related to each others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-fireplace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walmart_venv",
   "language": "python",
   "name": "walmart_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
