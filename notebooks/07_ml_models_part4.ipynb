{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a707337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# for making the output constant across all run\n",
    "np.random.seed(42)\n",
    "\n",
    "# display settings & code formatting\n",
    "pd.options.display.max_columns = 999\n",
    "%matplotlib inline\n",
    "\n",
    "# project paths\n",
    "project_root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "\n",
    "data_path = os.path.join(project_root_dir, \"data\")\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# function for loading data\n",
    "def read_data(filename, date_col=None, data_path=data_path):\n",
    "    csv_path = os.path.join(data_path, filename)\n",
    "    return pd.read_csv(csv_path, parse_dates=date_col)\n",
    "\n",
    "# function for saving data as csv file\n",
    "def save_dataframe(df, filename, file_path=data_path):\n",
    "    path = os.path.join(file_path, filename)\n",
    "    df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b922253",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data(\"train.csv\", date_col=[\"Date\"])\n",
    "test = read_data(\"test.csv\", date_col=[\"Date\"])\n",
    "stores = read_data(\"stores.csv\")\n",
    "features = read_data(\"features.csv\", date_col=[\"Date\"])\n",
    "sample_submission = read_data(\"sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2fd934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9741affa3036>:20: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  train[\"WeekOfYear\"] = train[\"Date\"].dt.weekofyear\n",
      "<ipython-input-3-9741affa3036>:27: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  test[\"WeekOfYear\"] = test[\"Date\"].dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "# Merge the stores data with train and test\n",
    "train = pd.merge(train, stores, how=\"left\", on=\"Store\")\n",
    "test = pd.merge(test, stores, how=\"left\", on=\"Store\")\n",
    "\n",
    "# Merge the features data with train and test\n",
    "train = pd.merge(train, features, how=\"left\", on=[\"Store\", \"Date\"])\n",
    "test = pd.merge(test, features, how=\"left\", on=[\"Store\", \"Date\"])\n",
    "\n",
    "train.drop([\"IsHoliday_y\"], axis=1, inplace=True)\n",
    "test.drop([\"IsHoliday_y\"], axis=1, inplace=True)\n",
    "\n",
    "# rename column\n",
    "train.rename(columns={\"IsHoliday_x\": \"IsHoliday\"}, inplace=True)\n",
    "test.rename(columns={\"IsHoliday_x\": \"IsHoliday\"}, inplace=True)\n",
    "\n",
    "## Datetime features\n",
    "train[\"Year\"] = train[\"Date\"].dt.year\n",
    "train[\"Month\"] = train[\"Date\"].dt.month\n",
    "train[\"Day\"] = train[\"Date\"].dt.day\n",
    "train[\"WeekOfYear\"] = train[\"Date\"].dt.weekofyear\n",
    "train[\"DayOfWeek\"] = train[\"Date\"].dt.dayofweek\n",
    "train[\"Weekend\"] = (train[\"Date\"].dt.weekday >= 5).astype(int)\n",
    "\n",
    "test[\"Year\"] = test[\"Date\"].dt.year\n",
    "test[\"Month\"] = test[\"Date\"].dt.month\n",
    "test[\"Day\"] = test[\"Date\"].dt.day\n",
    "test[\"WeekOfYear\"] = test[\"Date\"].dt.weekofyear\n",
    "test[\"DayOfWeek\"] = test[\"Date\"].dt.dayofweek\n",
    "test[\"Weekend\"] = (test[\"Date\"].dt.weekday >= 5).astype(int)\n",
    "\n",
    "# convert boolean column to categorical column\n",
    "train[\"IsHoliday\"] = train[\"IsHoliday\"].map({True: \"Yes\", False: \"No\"})\n",
    "test[\"IsHoliday\"] = test[\"IsHoliday\"].map({True: \"Yes\", False: \"No\"})\n",
    "train[\"IsHoliday\"] = train[\"IsHoliday\"].astype(\"category\")\n",
    "test[\"IsHoliday\"] = test[\"IsHoliday\"].astype(\"category\")\n",
    "\n",
    "# ordered the categorical store type col\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "cat_type = CategoricalDtype(categories=[\"C\", \"B\", \"A\"], ordered=True)\n",
    "train[\"Type\"] = train[\"Type\"].astype(cat_type)\n",
    "test[\"Type\"] = test[\"Type\"].astype(cat_type)\n",
    "\n",
    "# convert to categorical columns\n",
    "train[\"Store\"] = train[\"Store\"].astype(\"category\")\n",
    "train[\"Dept\"] = train[\"Dept\"].astype(\"category\")\n",
    "train[\"Year\"] = train[\"Year\"].astype(\"category\")\n",
    "train[\"Month\"] = train[\"Month\"].astype(\"category\")\n",
    "train[\"DayOfWeek\"] = train[\"DayOfWeek\"].astype(\"category\")\n",
    "train[\"Weekend\"] = train[\"Weekend\"].astype(\"category\")\n",
    "\n",
    "# convert to categorical columns\n",
    "test[\"Store\"] = test[\"Store\"].astype(\"category\")\n",
    "test[\"Dept\"] = test[\"Dept\"].astype(\"category\")\n",
    "test[\"Year\"] = test[\"Year\"].astype(\"category\")\n",
    "test[\"Month\"] = test[\"Month\"].astype(\"category\")\n",
    "test[\"DayOfWeek\"] = test[\"DayOfWeek\"].astype(\"category\")\n",
    "test[\"Weekend\"] = test[\"Weekend\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a349c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(strategy='median')),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 ['Size', 'Temperature', 'Fuel_Price',\n",
       "                                  'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
       "                                  'MarkDown4', 'MarkDown5', 'CPI',\n",
       "                                  'Unemployment', 'Day', 'WeekOfYear']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(fill_value='NA',\n",
       "                                                                strategy='constant')),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse=False))]),\n",
       "                                 ['Store', 'Dept', 'IsHoliday', 'Type', 'Year',\n",
       "                                  'Month', 'DayOfWeek', 'Weekend'])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# features and labels of train and test set\n",
    "# labels of test are not provided as we need to predict them\n",
    "\n",
    "X_train = train.drop([\"Weekly_Sales\"], axis=1).copy()\n",
    "y_train = train[\"Weekly_Sales\"].copy()\n",
    "\n",
    "X_test = test.copy()\n",
    "\n",
    "# drop and save the date column in a variable\n",
    "train_date = X_train.pop(\"Date\")\n",
    "test_date = X_test.pop(\"Date\")\n",
    "\n",
    "\n",
    "#### Data preparation pipeline\n",
    "\n",
    "# select numerical and categorical columns\n",
    "num_cols = X_train.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# numerical date preprocessing pipeline\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "# categorical data preprocessing pipeline\n",
    "cat_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    ")\n",
    "\n",
    "# full pipeline\n",
    "full_pipe = ColumnTransformer(\n",
    "    [(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)]\n",
    ")\n",
    "\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33012c85",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648d27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c851af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make submission\n",
    "y_pred = rf.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"rf_default_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbec996",
   "metadata": {},
   "source": [
    "The wmae on the public leaderbaord is `3188` and the ranking is within `260` and the private leaderbaord the wmae is `3326` and the position here is also within `260`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a378",
   "metadata": {},
   "source": [
    "### Plot Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41aa6bb",
   "metadata": {},
   "source": [
    "As the data set is bigger and scikit-learn doesn't support gpu, it has been observed before that training the model and finding good hyper-paramters values is getting very difficult,so we will only use fraction of the data for building and evaluting the model to speed up the process. For this, we will plot the learning curve to find the optimal training data we need to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fee20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 421570\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in the training set:\", X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fe9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(estimator, X, y, cv):\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        cv=cv,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        random_state=42\n",
    "    )\n",
    "    train_mean = np.mean(-train_scores, axis=1)\n",
    "    test_mean = np.mean(-test_scores, axis=1)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=train_sizes,\n",
    "            y=train_mean,\n",
    "            name=\"Training MAE\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=train_sizes,\n",
    "            y=test_mean,\n",
    "            name=\"Validation MAE\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Learning Curves\",\n",
    "        xaxis_title=\"Number of training examples\",\n",
    "        yaxis_title=\"Mean Absolute Error\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac16987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "plot_learning_curves(rf, X_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b522a",
   "metadata": {},
   "source": [
    "After around 84000 (20%) of the samples, the mae stoped decreasing and mostly become flat after that. So, to speed up the model selection and hyper-parameter optimization process we will randomly select only 30% of the data from the training set , and further divide it into 80-20 split between training and validation set. We can also see that the model is ovefitting badly, the error on training set is way less than the error on the validation set, so we also have to take care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ecf85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full = X_train.copy()\n",
    "y_train_full = y_train.copy()\n",
    "\n",
    "# randomly select 30% of the data only\n",
    "train = pd.concat([X_train, y_train], axis='columns')\n",
    "train = train.sample(frac=0.3, random_state=42)\n",
    "\n",
    "X_train = train.drop([\"Weekly_Sales\"], axis=1).copy()\n",
    "y_train = train[\"Weekly_Sales\"].copy()\n",
    "\n",
    "# now divide it to train and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5682809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 49s, sys: 4.92 s, total: 18min 54s\n",
      "Wall time: 5min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Size', 'Temperature',\n",
       "                                                   'Fuel_Price', 'MarkDown1',\n",
       "                                                   'MarkDown2', 'MarkDown3',\n",
       "                                                   'MarkDown4', 'MarkDown5',\n",
       "                                                   'CPI', 'Unemployment', 'Day',\n",
       "                                                   'WeekOfYear']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(fill_value='NA',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  ['Store', 'Dept', 'IsHoliday',\n",
       "                                                   'Type', 'Year', 'Month',\n",
       "                                                   'DayOfWeek', 'Weekend'])])),\n",
       "                ('randomforestregressor',\n",
       "                 RandomForestRegressor(n_jobs=-1, random_state=42))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a31f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make submission\n",
    "y_pred = rf.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"rf_default_reduced_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b166b",
   "metadata": {},
   "source": [
    "Although the wmae on the leaderboard is `3611` which is much higer than before which was `3188` but we can also see that the training time has also reduced significantly from `27 min to 5 min` only, which we need very badly.Once we find good hyper-parameter values for the models, we will train them again using the full data to get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c21f6",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ceee93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute errors: [1740.07452545 1781.81323016 1733.58122614 1804.84640702 1758.20869911]\n",
      "mean of mae 1763.7048175742482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "print(\"mean absolute errors:\", -scores)\n",
    "print(\"mean of mae\", np.mean(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a06fa",
   "metadata": {},
   "source": [
    "This is the score we will try to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5df714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from skopt.space import Real, Integer, Categorical\n",
    "# from skopt.utils import use_named_args\n",
    "# from skopt import gp_minimize\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # random forest model\n",
    "# rf = make_pipeline(full_pipe, RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "\n",
    "# # The list of hyper-parameters to optimize\n",
    "# space = [\n",
    "#     #Integer(low=100, high=1500, name=\"randomforestregressor__n_estimators\"),\n",
    "#     #Integer(low=5, high=20, name=\"randomforestregressor__max_depth\"),\n",
    "#     #Categorical([\"auto\",\"sqrt\",\"log2\"], name=\"randomforestregressor__max_features\")\n",
    "#     Categorical([\"mse\",\"mae\"], name=\"randomforestregressor__criterion\"),\n",
    "#     Integer(low=2, high=10, name=\"randomforestregressor__min_samples_split\"),\n",
    "#     Integer(low=1, high=10, name=\"randomforestregressor__min_samples_leaf\"),\n",
    "    \n",
    "# ]\n",
    "\n",
    "# @use_named_args(space)\n",
    "# def objective(**params):\n",
    "#     rf.set_params(**params)\n",
    "#     return -np.mean(cross_val_score(rf, X_train, y_train, cv=3,\n",
    "#                                     scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "# res_gp = gp_minimize(objective, space, n_calls=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e472430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV 1/2; 1/10] START randomforestregressor__min_samples_leaf=7, randomforestregressor__min_samples_split=5\n",
      "[CV 1/2; 1/10] END randomforestregressor__min_samples_leaf=7, randomforestregressor__min_samples_split=5;, score=-2281.560 total time= 2.6min\n",
      "[CV 2/2; 1/10] START randomforestregressor__min_samples_leaf=7, randomforestregressor__min_samples_split=5\n",
      "[CV 2/2; 1/10] END randomforestregressor__min_samples_leaf=7, randomforestregressor__min_samples_split=5;, score=-2314.329 total time= 2.8min\n",
      "[CV 1/2; 2/10] START randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6\n",
      "[CV 1/2; 2/10] END randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6;, score=-2377.817 total time= 2.6min\n",
      "[CV 2/2; 2/10] START randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6\n",
      "[CV 2/2; 2/10] END randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6;, score=-2401.002 total time= 2.4min\n",
      "[CV 1/2; 3/10] START randomforestregressor__min_samples_leaf=5, randomforestregressor__min_samples_split=8\n",
      "[CV 1/2; 3/10] END randomforestregressor__min_samples_leaf=5, randomforestregressor__min_samples_split=8;, score=-2127.101 total time= 2.6min\n",
      "[CV 2/2; 3/10] START randomforestregressor__min_samples_leaf=5, randomforestregressor__min_samples_split=8\n",
      "[CV 2/2; 3/10] END randomforestregressor__min_samples_leaf=5, randomforestregressor__min_samples_split=8;, score=-2153.954 total time= 2.6min\n",
      "[CV 1/2; 4/10] START randomforestregressor__min_samples_leaf=3, randomforestregressor__min_samples_split=8\n",
      "[CV 1/2; 4/10] END randomforestregressor__min_samples_leaf=3, randomforestregressor__min_samples_split=8;, score=-2035.866 total time= 2.8min\n",
      "[CV 2/2; 4/10] START randomforestregressor__min_samples_leaf=3, randomforestregressor__min_samples_split=8\n",
      "[CV 2/2; 4/10] END randomforestregressor__min_samples_leaf=3, randomforestregressor__min_samples_split=8;, score=-2055.512 total time= 2.8min\n",
      "[CV 1/2; 5/10] START randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6\n",
      "[CV 1/2; 5/10] END randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6;, score=-2377.817 total time= 2.7min\n",
      "[CV 2/2; 5/10] START randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6\n",
      "[CV 2/2; 5/10] END randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=6;, score=-2401.002 total time= 2.6min\n",
      "[CV 1/2; 6/10] START randomforestregressor__min_samples_leaf=4, randomforestregressor__min_samples_split=9\n",
      "[CV 1/2; 6/10] END randomforestregressor__min_samples_leaf=4, randomforestregressor__min_samples_split=9;, score=-2080.136 total time= 2.7min\n",
      "[CV 2/2; 6/10] START randomforestregressor__min_samples_leaf=4, randomforestregressor__min_samples_split=9\n",
      "[CV 2/2; 6/10] END randomforestregressor__min_samples_leaf=4, randomforestregressor__min_samples_split=9;, score=-2099.607 total time= 2.6min\n",
      "[CV 1/2; 7/10] START randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=4\n",
      "[CV 1/2; 7/10] END randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=4;, score=-2377.817 total time= 2.8min\n",
      "[CV 2/2; 7/10] START randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=4\n",
      "[CV 2/2; 7/10] END randomforestregressor__min_samples_leaf=8, randomforestregressor__min_samples_split=4;, score=-2401.002 total time= 2.5min\n",
      "[CV 1/2; 8/10] START randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=6\n",
      "[CV 1/2; 8/10] END randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=6;, score=-2196.157 total time= 2.7min\n",
      "[CV 2/2; 8/10] START randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=6\n",
      "[CV 2/2; 8/10] END randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=6;, score=-2230.804 total time= 2.8min\n",
      "[CV 1/2; 9/10] START randomforestregressor__min_samples_leaf=2, randomforestregressor__min_samples_split=9\n",
      "[CV 1/2; 9/10] END randomforestregressor__min_samples_leaf=2, randomforestregressor__min_samples_split=9;, score=-2024.379 total time= 2.5min\n",
      "[CV 2/2; 9/10] START randomforestregressor__min_samples_leaf=2, randomforestregressor__min_samples_split=9\n",
      "[CV 2/2; 9/10] END randomforestregressor__min_samples_leaf=2, randomforestregressor__min_samples_split=9;, score=-2050.952 total time= 2.5min\n",
      "[CV 1/2; 10/10] START randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=3\n",
      "[CV 1/2; 10/10] END randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=3;, score=-2196.157 total time= 2.7min\n",
      "[CV 2/2; 10/10] START randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=3\n",
      "[CV 2/2; 10/10] END randomforestregressor__min_samples_leaf=6, randomforestregressor__min_samples_split=3;, score=-2230.804 total time= 2.8min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('columntransformer',\n",
       "                                              ColumnTransformer(transformers=[('num',\n",
       "                                                                               Pipeline(steps=[('simpleimputer',\n",
       "                                                                                                SimpleImputer(strategy='median')),\n",
       "                                                                                               ('standardscaler',\n",
       "                                                                                                StandardScaler())]),\n",
       "                                                                               ['Size',\n",
       "                                                                                'Temperature',\n",
       "                                                                                'Fuel_Price',\n",
       "                                                                                'MarkDown1',\n",
       "                                                                                'MarkDown2',\n",
       "                                                                                'MarkDown3',\n",
       "                                                                                'MarkDown4',\n",
       "                                                                                'MarkDown5',\n",
       "                                                                                'CPI',\n",
       "                                                                                'Unemployment',\n",
       "                                                                                'Day',\n",
       "                                                                                'WeekOfYear']...\n",
       "                                              RandomForestRegressor(n_jobs=-1,\n",
       "                                                                    random_state=42))]),\n",
       "                   param_distributions={'randomforestregressor__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x129b11070>,\n",
       "                                        'randomforestregressor__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x129b11340>},\n",
       "                   random_state=42, scoring='neg_mean_absolute_error',\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "params = {\n",
    "    \"randomforestregressor__min_samples_split\": randint(low=2, high=10),\n",
    "    \"randomforestregressor__min_samples_leaf\": randint(low=1, high=10), \n",
    "}\n",
    "\n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "\n",
    "rf_rnd_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions= params,\n",
    "    n_iter=10,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=2,\n",
    "    verbose=10,\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "rf_rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f8c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2037.6653006070487"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rnd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5351658d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__min_samples_leaf': 2,\n",
       " 'randomforestregressor__min_samples_split': 9}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48205b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n",
      "[CV 1/2] END randomforestregressor__max_depth=5, randomforestregressor__max_features=auto;, score=-11521.008 total time=  34.6s\n",
      "[CV 2/2] END randomforestregressor__max_depth=5, randomforestregressor__max_features=auto;, score=-11431.183 total time=  32.3s\n",
      "[CV 1/2] END randomforestregressor__max_depth=5, randomforestregressor__max_features=sqrt;, score=-12946.335 total time=   4.8s\n",
      "[CV 2/2] END randomforestregressor__max_depth=5, randomforestregressor__max_features=sqrt;, score=-13012.978 total time=   4.8s\n",
      "[CV 1/2] END randomforestregressor__max_depth=5, randomforestregressor__max_features=log2;, score=-13484.995 total time=   3.4s\n",
      "[CV 2/2] END randomforestregressor__max_depth=5, randomforestregressor__max_features=log2;, score=-13534.485 total time=   3.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=6, randomforestregressor__max_features=auto;, score=-10889.150 total time=  36.9s\n",
      "[CV 2/2] END randomforestregressor__max_depth=6, randomforestregressor__max_features=auto;, score=-10767.863 total time=  36.0s\n",
      "[CV 1/2] END randomforestregressor__max_depth=6, randomforestregressor__max_features=sqrt;, score=-12619.168 total time=   5.0s\n",
      "[CV 2/2] END randomforestregressor__max_depth=6, randomforestregressor__max_features=sqrt;, score=-12625.950 total time=   5.2s\n",
      "[CV 1/2] END randomforestregressor__max_depth=6, randomforestregressor__max_features=log2;, score=-13157.807 total time=   3.7s\n",
      "[CV 2/2] END randomforestregressor__max_depth=6, randomforestregressor__max_features=log2;, score=-13257.717 total time=   3.8s\n",
      "[CV 1/2] END randomforestregressor__max_depth=7, randomforestregressor__max_features=auto;, score=-10317.370 total time=  41.7s\n",
      "[CV 2/2] END randomforestregressor__max_depth=7, randomforestregressor__max_features=auto;, score=-10236.674 total time=  39.0s\n",
      "[CV 1/2] END randomforestregressor__max_depth=7, randomforestregressor__max_features=sqrt;, score=-12140.306 total time=   5.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=7, randomforestregressor__max_features=sqrt;, score=-12237.518 total time=   5.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=7, randomforestregressor__max_features=log2;, score=-12845.374 total time=   3.9s\n",
      "[CV 2/2] END randomforestregressor__max_depth=7, randomforestregressor__max_features=log2;, score=-12962.115 total time=   4.0s\n",
      "[CV 1/2] END randomforestregressor__max_depth=8, randomforestregressor__max_features=auto;, score=-9689.206 total time=  44.4s\n",
      "[CV 2/2] END randomforestregressor__max_depth=8, randomforestregressor__max_features=auto;, score=-9716.234 total time=  44.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=8, randomforestregressor__max_features=sqrt;, score=-11784.890 total time=   5.9s\n",
      "[CV 2/2] END randomforestregressor__max_depth=8, randomforestregressor__max_features=sqrt;, score=-11919.534 total time=   5.9s\n",
      "[CV 1/2] END randomforestregressor__max_depth=8, randomforestregressor__max_features=log2;, score=-12582.587 total time=   4.6s\n",
      "[CV 2/2] END randomforestregressor__max_depth=8, randomforestregressor__max_features=log2;, score=-12661.429 total time=   4.6s\n",
      "[CV 1/2] END randomforestregressor__max_depth=9, randomforestregressor__max_features=auto;, score=-9158.775 total time=  49.6s\n",
      "[CV 2/2] END randomforestregressor__max_depth=9, randomforestregressor__max_features=auto;, score=-9252.308 total time=  56.3s\n",
      "[CV 1/2] END randomforestregressor__max_depth=9, randomforestregressor__max_features=sqrt;, score=-11456.783 total time=   6.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=9, randomforestregressor__max_features=sqrt;, score=-11529.010 total time=   7.6s\n",
      "[CV 1/2] END randomforestregressor__max_depth=9, randomforestregressor__max_features=log2;, score=-12339.676 total time=   5.6s\n",
      "[CV 2/2] END randomforestregressor__max_depth=9, randomforestregressor__max_features=log2;, score=-12415.608 total time=   4.9s\n",
      "[CV 1/2] END randomforestregressor__max_depth=10, randomforestregressor__max_features=auto;, score=-8665.847 total time=  54.8s\n",
      "[CV 2/2] END randomforestregressor__max_depth=10, randomforestregressor__max_features=auto;, score=-8733.325 total time= 1.0min\n",
      "[CV 1/2] END randomforestregressor__max_depth=10, randomforestregressor__max_features=sqrt;, score=-11115.130 total time=   7.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=10, randomforestregressor__max_features=sqrt;, score=-11169.744 total time=   7.7s\n",
      "[CV 1/2] END randomforestregressor__max_depth=10, randomforestregressor__max_features=log2;, score=-12059.456 total time=   5.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=10, randomforestregressor__max_features=log2;, score=-12088.793 total time=   5.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=11, randomforestregressor__max_features=auto;, score=-8190.950 total time=  58.1s\n",
      "[CV 2/2] END randomforestregressor__max_depth=11, randomforestregressor__max_features=auto;, score=-8286.776 total time=  55.9s\n",
      "[CV 1/2] END randomforestregressor__max_depth=11, randomforestregressor__max_features=sqrt;, score=-10795.480 total time=   7.3s\n",
      "[CV 2/2] END randomforestregressor__max_depth=11, randomforestregressor__max_features=sqrt;, score=-10824.657 total time=   7.2s\n",
      "[CV 1/2] END randomforestregressor__max_depth=11, randomforestregressor__max_features=log2;, score=-11802.621 total time=   5.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=11, randomforestregressor__max_features=log2;, score=-11915.442 total time=   5.5s\n",
      "[CV 1/2] END randomforestregressor__max_depth=12, randomforestregressor__max_features=auto;, score=-7830.995 total time=  59.9s\n",
      "[CV 2/2] END randomforestregressor__max_depth=12, randomforestregressor__max_features=auto;, score=-7878.360 total time= 1.1min\n",
      "[CV 1/2] END randomforestregressor__max_depth=12, randomforestregressor__max_features=sqrt;, score=-10378.331 total time=   8.9s\n",
      "[CV 2/2] END randomforestregressor__max_depth=12, randomforestregressor__max_features=sqrt;, score=-10428.875 total time=   8.5s\n",
      "[CV 1/2] END randomforestregressor__max_depth=12, randomforestregressor__max_features=log2;, score=-11530.937 total time=   6.0s\n",
      "[CV 2/2] END randomforestregressor__max_depth=12, randomforestregressor__max_features=log2;, score=-11550.376 total time=   6.2s\n",
      "[CV 1/2] END randomforestregressor__max_depth=13, randomforestregressor__max_features=auto;, score=-7468.008 total time= 1.2min\n",
      "[CV 2/2] END randomforestregressor__max_depth=13, randomforestregressor__max_features=auto;, score=-7491.831 total time= 1.2min\n",
      "[CV 1/2] END randomforestregressor__max_depth=13, randomforestregressor__max_features=sqrt;, score=-10010.355 total time=   9.0s\n",
      "[CV 2/2] END randomforestregressor__max_depth=13, randomforestregressor__max_features=sqrt;, score=-10102.025 total time=   8.7s\n",
      "[CV 1/2] END randomforestregressor__max_depth=13, randomforestregressor__max_features=log2;, score=-11287.511 total time=   6.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=13, randomforestregressor__max_features=log2;, score=-11249.169 total time=   6.3s\n",
      "[CV 1/2] END randomforestregressor__max_depth=14, randomforestregressor__max_features=auto;, score=-7128.057 total time= 1.2min\n",
      "[CV 2/2] END randomforestregressor__max_depth=14, randomforestregressor__max_features=auto;, score=-7180.249 total time= 1.2min\n",
      "[CV 1/2] END randomforestregressor__max_depth=14, randomforestregressor__max_features=sqrt;, score=-9781.316 total time=   9.4s\n",
      "[CV 2/2] END randomforestregressor__max_depth=14, randomforestregressor__max_features=sqrt;, score=-9789.087 total time=   9.0s\n",
      "[CV 1/2] END randomforestregressor__max_depth=14, randomforestregressor__max_features=log2;, score=-10924.552 total time=   6.7s\n",
      "[CV 2/2] END randomforestregressor__max_depth=14, randomforestregressor__max_features=log2;, score=-11105.743 total time=   6.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=15, randomforestregressor__max_features=auto;, score=-6837.903 total time= 1.2min\n",
      "[CV 2/2] END randomforestregressor__max_depth=15, randomforestregressor__max_features=auto;, score=-6879.242 total time= 1.3min\n",
      "[CV 1/2] END randomforestregressor__max_depth=15, randomforestregressor__max_features=sqrt;, score=-9519.110 total time=   9.4s\n",
      "[CV 2/2] END randomforestregressor__max_depth=15, randomforestregressor__max_features=sqrt;, score=-9584.534 total time=   9.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END randomforestregressor__max_depth=15, randomforestregressor__max_features=log2;, score=-10723.403 total time=   7.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=15, randomforestregressor__max_features=log2;, score=-10672.791 total time=   7.1s\n",
      "[CV 1/2] END randomforestregressor__max_depth=16, randomforestregressor__max_features=auto;, score=-6522.411 total time= 1.3min\n",
      "[CV 2/2] END randomforestregressor__max_depth=16, randomforestregressor__max_features=auto;, score=-6581.521 total time= 1.4min\n",
      "[CV 1/2] END randomforestregressor__max_depth=16, randomforestregressor__max_features=sqrt;, score=-9183.458 total time=   9.7s\n",
      "[CV 2/2] END randomforestregressor__max_depth=16, randomforestregressor__max_features=sqrt;, score=-9268.748 total time=   9.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=16, randomforestregressor__max_features=log2;, score=-10429.448 total time=   9.8s\n",
      "[CV 2/2] END randomforestregressor__max_depth=16, randomforestregressor__max_features=log2;, score=-10523.738 total time=   7.2s\n",
      "[CV 1/2] END randomforestregressor__max_depth=17, randomforestregressor__max_features=auto;, score=-6277.454 total time= 1.4min\n",
      "[CV 2/2] END randomforestregressor__max_depth=17, randomforestregressor__max_features=auto;, score=-6301.379 total time= 1.4min\n",
      "[CV 1/2] END randomforestregressor__max_depth=17, randomforestregressor__max_features=sqrt;, score=-8863.590 total time=   9.7s\n",
      "[CV 2/2] END randomforestregressor__max_depth=17, randomforestregressor__max_features=sqrt;, score=-8985.411 total time=  12.2s\n",
      "[CV 1/2] END randomforestregressor__max_depth=17, randomforestregressor__max_features=log2;, score=-10221.953 total time=  10.9s\n",
      "[CV 2/2] END randomforestregressor__max_depth=17, randomforestregressor__max_features=log2;, score=-10329.361 total time=   9.9s\n",
      "[CV 1/2] END randomforestregressor__max_depth=18, randomforestregressor__max_features=auto;, score=-6019.946 total time= 1.4min\n",
      "[CV 2/2] END randomforestregressor__max_depth=18, randomforestregressor__max_features=auto;, score=-6064.783 total time= 1.4min\n",
      "[CV 1/2] END randomforestregressor__max_depth=18, randomforestregressor__max_features=sqrt;, score=-8659.296 total time=  11.7s\n",
      "[CV 2/2] END randomforestregressor__max_depth=18, randomforestregressor__max_features=sqrt;, score=-8707.352 total time=  10.8s\n",
      "[CV 1/2] END randomforestregressor__max_depth=18, randomforestregressor__max_features=log2;, score=-9951.898 total time=   7.8s\n",
      "[CV 2/2] END randomforestregressor__max_depth=18, randomforestregressor__max_features=log2;, score=-10050.494 total time=   8.0s\n",
      "[CV 1/2] END randomforestregressor__max_depth=19, randomforestregressor__max_features=auto;, score=-5768.014 total time= 1.5min\n",
      "[CV 2/2] END randomforestregressor__max_depth=19, randomforestregressor__max_features=auto;, score=-5819.856 total time= 1.4min\n",
      "[CV 1/2] END randomforestregressor__max_depth=19, randomforestregressor__max_features=sqrt;, score=-8351.075 total time=  10.1s\n",
      "[CV 2/2] END randomforestregressor__max_depth=19, randomforestregressor__max_features=sqrt;, score=-8489.191 total time=  10.2s\n",
      "[CV 1/2] END randomforestregressor__max_depth=19, randomforestregressor__max_features=log2;, score=-9652.821 total time=   7.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=19, randomforestregressor__max_features=log2;, score=-9818.302 total time=   7.5s\n",
      "[CV 1/2] END randomforestregressor__max_depth=20, randomforestregressor__max_features=auto;, score=-5546.426 total time= 1.4min\n",
      "[CV 2/2] END randomforestregressor__max_depth=20, randomforestregressor__max_features=auto;, score=-5591.659 total time= 1.4min\n",
      "[CV 1/2] END randomforestregressor__max_depth=20, randomforestregressor__max_features=sqrt;, score=-8215.350 total time=  10.5s\n",
      "[CV 2/2] END randomforestregressor__max_depth=20, randomforestregressor__max_features=sqrt;, score=-8243.944 total time=  10.4s\n",
      "[CV 1/2] END randomforestregressor__max_depth=20, randomforestregressor__max_features=log2;, score=-9485.574 total time=   8.2s\n",
      "[CV 2/2] END randomforestregressor__max_depth=20, randomforestregressor__max_features=log2;, score=-9657.129 total time=   7.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('standardscaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['Size',\n",
       "                                                                          'Temperature',\n",
       "                                                                          'Fuel_Price',\n",
       "                                                                          'MarkDown1',\n",
       "                                                                          'MarkDown2',\n",
       "                                                                          'MarkDown3',\n",
       "                                                                          'MarkDown4',\n",
       "                                                                          'MarkDown5',\n",
       "                                                                          'CPI',\n",
       "                                                                          'Unemployment',\n",
       "                                                                          'Day',\n",
       "                                                                          'WeekOfYear']),\n",
       "                                                                        ('ca...\n",
       "                                                                          'Year',\n",
       "                                                                          'Month',\n",
       "                                                                          'DayOfWeek',\n",
       "                                                                          'Weekend'])])),\n",
       "                                       ('randomforestregressor',\n",
       "                                        RandomForestRegressor(min_samples_leaf=2,\n",
       "                                                              min_samples_split=9,\n",
       "                                                              n_jobs=-1,\n",
       "                                                              random_state=42))]),\n",
       "             param_grid={'randomforestregressor__max_depth': [5, 6, 7, 8, 9, 10,\n",
       "                                                              11, 12, 13, 14,\n",
       "                                                              15, 16, 17, 18,\n",
       "                                                              19, 20],\n",
       "                         'randomforestregressor__max_features': ['auto', 'sqrt',\n",
       "                                                                 'log2']},\n",
       "             scoring='neg_mean_absolute_error', verbose=4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"randomforestregressor__max_depth\": list(range(5,21)), \n",
    "    \"randomforestregressor__max_features\": [\"auto\",\"sqrt\",\"log2\"]\n",
    "}\n",
    "\n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(random_state=42, \n",
    "                                                    min_samples_leaf= 2, \n",
    "                                                    min_samples_split = 9,\n",
    "                                                    n_jobs=-1))\n",
    "\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator= rf,\n",
    "    param_grid= params,\n",
    "    scoring= \"neg_mean_absolute_error\",\n",
    "    cv=2,\n",
    "    verbose=4,    \n",
    ")\n",
    "\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1eb05c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': 20,\n",
       " 'randomforestregressor__max_features': 'auto'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a7ad9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5569.0428275555805"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe13fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 14 candidates, totalling 28 fits\n",
      "[CV 1/2] END randomforestregressor__n_estimators=100;, score=-2024.379 total time= 2.6min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=100;, score=-2050.952 total time= 2.5min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=200;, score=-2018.020 total time= 5.1min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=200;, score=-2046.180 total time= 5.0min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=300;, score=-2014.191 total time= 7.5min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=300;, score=-2046.790 total time= 7.4min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=400;, score=-2014.730 total time=13.1min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=400;, score=-2046.517 total time=12.2min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=500;, score=-2013.453 total time=13.7min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=500;, score=-2045.022 total time=13.6min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=600;, score=-2013.679 total time=17.1min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=600;, score=-2044.967 total time=17.1min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=700;, score=-2013.504 total time=17.9min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=700;, score=-2044.455 total time=17.0min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=800;, score=-2012.822 total time=19.7min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=800;, score=-2044.486 total time=19.5min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=900;, score=-2013.021 total time=22.0min\n",
      "[CV 2/2] END randomforestregressor__n_estimators=900;, score=-2044.678 total time=21.8min\n",
      "[CV 1/2] END randomforestregressor__n_estimators=1000;, score=-2013.700 total time=24.2min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"randomforestregressor__n_estimators\": list(range(100,1500,100)), \n",
    "}\n",
    "\n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(random_state=42, \n",
    "                                                    min_samples_leaf= 2, \n",
    "                                                    min_samples_split = 9,\n",
    "                                                    max_features=\"auto\",\n",
    "                                                    n_jobs=-1))\n",
    "\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator= rf,\n",
    "    param_grid= params,\n",
    "    scoring= \"neg_mean_absolute_error\",\n",
    "    cv=2,\n",
    "    verbose=4,    \n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb4a6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute errors: [1791.46104239 1843.11797992 1782.74458526 1865.49642173 1811.753268  ]\n",
      "mean of mae 1818.9146594617919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(min_samples_leaf=2,\n",
    "                                                    min_samples_split=9,\n",
    "                                                    random_state=42,\n",
    "                                                    n_jobs=-1\n",
    "                                                   ))\n",
    "\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "print(\"mean absolute errors:\", -scores)\n",
    "print(\"mean of mae\", np.mean(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1f53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the rf tunned model on all of the data \n",
    "rf = make_pipeline(full_pipe, RandomForestRegressor(min_samples_leaf=2,\n",
    "                                                    min_samples_split=9,\n",
    "                                                    random_state=42,\n",
    "                                                    n_jobs=-1\n",
    "                                                   ))\n",
    "rf.fit(X_train_full, y_train_full)\n",
    "\n",
    "# make submission\n",
    "y_pred = rf.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"rf_hp_tunned_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc91360",
   "metadata": {},
   "source": [
    "The rf model with default hyper paramters is still doing better than this model, so we will stick with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24ed3b",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d09631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = make_pipeline(full_pipe, GradientBoostingRegressor(random_state=42))\n",
    "gbrt.fit(X_train_full, y_train_full)\n",
    "\n",
    "# make submission\n",
    "y_pred = gbrt.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"gbrt_default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67d8fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, val_error: 8016.77935995346\n",
      "n_estimators: 150, val_error: 8016.77935995346\n",
      "n_estimators: 200, val_error: 8016.77935995346\n",
      "n_estimators: 250, val_error: 8016.77935995346\n",
      "n_estimators: 300, val_error: 8016.77935995346\n",
      "n_estimators: 350, val_error: 8016.77935995346\n",
      "n_estimators: 400, val_error: 8016.77935995346\n",
      "n_estimators: 450, val_error: 8016.77935995346\n",
      "n_estimators: 500, val_error: 8016.77935995346\n",
      "n_estimators: 550, val_error: 8016.77935995346\n",
      "n_estimators: 600, val_error: 8016.77935995346\n"
     ]
    }
   ],
   "source": [
    "# with early stopping\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "gbrt = make_pipeline(full_pipe, GradientBoostingRegressor(random_state=42, warm_start=True))\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(100, 3050, 50):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_valid)\n",
    "    val_error = mean_absolute_error(y_valid, y_pred)\n",
    "    print(f\"n_estimators: {n_estimators}, val_error: {val_error}\")\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 10:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52a4622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('standardscaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['Size',\n",
       "                                                                          'Temperature',\n",
       "                                                                          'Fuel_Price',\n",
       "                                                                          'MarkDown1',\n",
       "                                                                          'MarkDown2',\n",
       "                                                                          'MarkDown3',\n",
       "                                                                          'MarkDown4',\n",
       "                                                                          'MarkDown5',\n",
       "                                                                          'CPI',\n",
       "                                                                          'Unemployment',\n",
       "                                                                          'Day',\n",
       "                                                                          'WeekOfYear']),\n",
       "                                                                        ('ca...\n",
       "                                                                          'IsHoliday',\n",
       "                                                                          'Type',\n",
       "                                                                          'Year',\n",
       "                                                                          'Month',\n",
       "                                                                          'DayOfWeek',\n",
       "                                                                          'Weekend'])])),\n",
       "                                       ('gradientboostingregressor',\n",
       "                                        GradientBoostingRegressor(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gradientboostingregressor__learning_rate': [0.0001,\n",
       "                                                                      0.001,\n",
       "                                                                      0.01,\n",
       "                                                                      0.015,\n",
       "                                                                      0.025,\n",
       "                                                                      0.05, 0.1,\n",
       "                                                                      0.2,\n",
       "                                                                      0.3],\n",
       "                         'gradientboostingregressor__loss': ['ls', 'lad',\n",
       "                                                             'huber',\n",
       "                                                             'quantile']},\n",
       "             scoring='neg_mean_absolute_error', verbose=4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbrt = make_pipeline(full_pipe, GradientBoostingRegressor(random_state=42))\n",
    "\n",
    "params = {\n",
    "    \"gradientboostingregressor__learning_rate\":[0.0001, 0.001, 0.01, 0.015, 0.025, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"gradientboostingregressor__loss\": [\"ls\", \"lad\", \"huber\", \"quantile\"]\n",
    "}\n",
    "\n",
    "gbrt_grid = GridSearchCV(\n",
    "    estimator= gbrt,\n",
    "    param_grid = params,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=4\n",
    ")\n",
    "gbrt_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59f3df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5598.5188403513885"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8915a5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingregressor__learning_rate': 0.3,\n",
       " 'gradientboostingregressor__loss': 'ls'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c34ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('standardscaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['Size',\n",
       "                                                                          'Temperature',\n",
       "                                                                          'Fuel_Price',\n",
       "                                                                          'MarkDown1',\n",
       "                                                                          'MarkDown2',\n",
       "                                                                          'MarkDown3',\n",
       "                                                                          'MarkDown4',\n",
       "                                                                          'MarkDown5',\n",
       "                                                                          'CPI',\n",
       "                                                                          'Unemployment',\n",
       "                                                                          'Day',\n",
       "                                                                          'WeekOfYear']),\n",
       "                                                                        ('ca...\n",
       "                                                                                          OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                                        sparse=False))]),\n",
       "                                                                         ['Store',\n",
       "                                                                          'Dept',\n",
       "                                                                          'IsHoliday',\n",
       "                                                                          'Type',\n",
       "                                                                          'Year',\n",
       "                                                                          'Month',\n",
       "                                                                          'DayOfWeek',\n",
       "                                                                          'Weekend'])])),\n",
       "                                       ('gradientboostingregressor',\n",
       "                                        GradientBoostingRegressor(n_estimators=1000,\n",
       "                                                                  random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gradientboostingregressor__learning_rate': [0.3, 0.4,\n",
       "                                                                      0.5, 0.6,\n",
       "                                                                      0.7,\n",
       "                                                                      0.8]},\n",
       "             scoring='neg_mean_absolute_error', verbose=4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbrt = make_pipeline(full_pipe, GradientBoostingRegressor(random_state=42, n_estimators=1000))\n",
    "\n",
    "params = {\n",
    "    \"gradientboostingregressor__learning_rate\":[0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "}\n",
    "\n",
    "gbrt_grid = GridSearchCV(\n",
    "    estimator= gbrt,\n",
    "    param_grid = params,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=4\n",
    ")\n",
    "gbrt_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28bc3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingregressor__learning_rate': 0.8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e87bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2739.6919045060217"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b1b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = make_pipeline(full_pipe, GradientBoostingRegressor(random_state=42, learning_rate=0.8))\n",
    "gbrt.fit(X_train_full, y_train_full)\n",
    "\n",
    "# make submission\n",
    "y_pred = gbrt.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"gbrt_testing_lr2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb289432",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d2d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Size', 'Temperature',\n",
       "                                                   'Fuel_Price', 'MarkDown1',\n",
       "                                                   'MarkDown2', 'MarkDown3',\n",
       "                                                   'MarkDown4', 'MarkDown5',\n",
       "                                                   'CPI', 'Unemployment', 'Day',\n",
       "                                                   'WeekOfYear']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(fill_value='NA',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  ['Store', 'Dept', 'IsHoliday',\n",
       "                                                   'Type', 'Year', 'Month',\n",
       "                                                   'DayOfWeek', 'Weekend'])])),\n",
       "                ('baggingregressor',\n",
       "                 BaggingRegressor(base_estimator=RandomForestRegressor(),\n",
       "                                  max_samples=5000, n_estimators=200, n_jobs=-1,\n",
       "                                  random_state=42))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "bag_reg = make_pipeline(full_pipe, BaggingRegressor(base_estimator= RandomForestRegressor(),\n",
    "                                                    n_estimators=200,\n",
    "                                                    max_samples=5000,\n",
    "                                                    bootstrap= True,\n",
    "                                                    random_state=42,\n",
    "                                                    n_jobs=-1))\n",
    "bag_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22df980e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bholaprasad/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# make submission\n",
    "y_pred = bag_reg.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"bagging_rf_samples5000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5368767",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52601a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Size', 'Temperature',\n",
       "                                                   'Fuel_Price', 'MarkDown1',\n",
       "                                                   'MarkDown2', 'MarkDown3',\n",
       "                                                   'MarkDown4', 'MarkDown5',\n",
       "                                                   'CPI', 'Unemployment', 'Day',\n",
       "                                                   'WeekOfYear']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(fill_value='NA',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  ['Store', 'Dept', 'IsHoliday',\n",
       "                                                   'Type', 'Year', 'Month',\n",
       "                                                   'DayOfWeek', 'Weekend'])])),\n",
       "                ('histgradientboostingregressor',\n",
       "                 HistGradientBoostingRegressor(early_stopping=True,\n",
       "                                               random_state=42))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hist_gbrt = make_pipeline(full_pipe, HistGradientBoostingRegressor(random_state=42, early_stopping=True))\n",
    "\n",
    "hist_gbrt.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ad2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make submission\n",
    "y_pred = hist_gbrt.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"hist_gbrt_sklearn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61e6a4",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567b3e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Size', 'Temperature',\n",
       "                                                   'Fuel_Price', 'MarkDown1',\n",
       "                                                   'MarkDown2', 'MarkDown3',\n",
       "                                                   'MarkDown4', 'MarkDown5',\n",
       "                                                   'CPI', 'Unemployment', 'Day',\n",
       "                                                   'WeekOfYear']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(fill_value='NA',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse=False))]),\n",
       "                                                  ['Store', 'Dept', 'IsHoliday',\n",
       "                                                   'Type', 'Year', 'Month',\n",
       "                                                   'DayOfWeek', 'Weekend'])])),\n",
       "                ('extratreesregressor', ExtraTreesRegressor(random_state=42))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "extra_tree = make_pipeline(full_pipe, ExtraTreesRegressor(random_state=42))\n",
    "extra_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19899c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make submission\n",
    "y_pred = extra_tree.predict(X_test)\n",
    "sample_submission[\"Weekly_Sales\"] = y_pred\n",
    "save_dataframe(sample_submission, \"extra_tree_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c459bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"extratreesregressor__min_samples_split\": list(range(2,10)),\n",
    "}\n",
    "\n",
    "extra_tree = make_pipeline(full_pipe, ExtraTreesRegressor(random_state=42))\n",
    "\n",
    "extree_grid = GridSearchCV(\n",
    "    estimator= extra_tree,\n",
    "    param_grid = params,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=2,\n",
    "    verbose=6,\n",
    "    n_jobs=-1\n",
    "    \n",
    ")\n",
    "\n",
    "extree_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db18ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walmart_venv",
   "language": "python",
   "name": "walmart_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
